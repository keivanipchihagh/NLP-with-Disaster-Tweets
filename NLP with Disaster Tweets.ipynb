{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Natural Language Processing with Disaster Tweets","metadata":{}},{"cell_type":"markdown","source":"## 1.Load Data & Libraries","metadata":{}},{"cell_type":"code","source":"import re                                                 # Regex for string extraction\nimport spacy                                              # Spacy for text processing\nimport string                                             # For using string functions and properties\nimport numpy as np                                        # Linear Algebra\nimport pandas as pd                                       # Pandas for DataFrame\nimport seaborn as sns                                     # Sns for plots\nfrom matplotlib import pyplot as plt                      # Matplotlib for plots\nfrom sklearn.model_selection import train_test_split      # Used to split the data into training, validation and test sets\nfrom wordcloud import WordCloud                           # Wordclouds\n\n# Keras packages\nfrom keras.preprocessing.text import Tokenizer            # Simplifies different tokenizing methods\nfrom keras.utils.vis_utils import plot_model              # For plotting model\nfrom keras.models import Sequential                       # Sequential model\nfrom keras.layers import Dense                            # Dense layer\n\nnlp = spacy.load('en_core_web_sm')  # Load the English language (Other options: en_core_web_md, en_core_web_lg)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-09-04T14:31:05.178181Z","iopub.execute_input":"2021-09-04T14:31:05.178566Z","iopub.status.idle":"2021-09-04T14:31:06.224043Z","shell.execute_reply.started":"2021-09-04T14:31:05.17853Z","shell.execute_reply":"2021-09-04T14:31:06.223008Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Load original data\ntrain_df = pd.read_csv('../input/nlp-getting-started/train.csv')\ntest_df = pd.read_csv('../input/nlp-getting-started/test.csv')\n\n# Combine original data for easier manipulating them later\ncombined = [train_df, test_df]\n\n# Take a leak at the original data\ntrain_df.head(5)","metadata":{"execution":{"iopub.status.busy":"2021-09-04T14:31:06.225587Z","iopub.execute_input":"2021-09-04T14:31:06.225905Z","iopub.status.idle":"2021-09-04T14:31:06.279558Z","shell.execute_reply.started":"2021-09-04T14:31:06.225866Z","shell.execute_reply":"2021-09-04T14:31:06.278879Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"I'll also load an external dataset to help speedup city and country extraction:","metadata":{}},{"cell_type":"code","source":"# Load cities data\ncities_df = pd.read_csv('../input/world-cities/worldcities.csv')\ncities_df['city'] = cities_df['city'].apply(lambda x: x.lower())\n\n# Get a list of cities\ncities = list(cities_df['city'])\n\n# Create country_df\ncountries_df = cities_df[['iso2', 'iso3', 'country']].groupby(by = 'country', as_index = False).max()\ncountries_df.dropna(axis = 0, inplace = True)                                 # Drop NAs\ncountries_df['iso2'] = countries_df['iso2'].apply(lambda x: x.lower())\niso2 = list(countries_df['iso2'])","metadata":{"execution":{"iopub.status.busy":"2021-09-04T14:31:06.280901Z","iopub.execute_input":"2021-09-04T14:31:06.281296Z","iopub.status.idle":"2021-09-04T14:31:06.532006Z","shell.execute_reply.started":"2021-09-04T14:31:06.281266Z","shell.execute_reply":"2021-09-04T14:31:06.531204Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Print basic properties\nprint(f\"- Training Data has {train_df.shape[0]} rows and {train_df.shape[1]} columns: ({list(train_df.columns)})\")\nprint(f\"- Testing Data has {test_df.shape[0]} rows and {test_df.shape[1]} columns: ({list(test_df.columns)})\")","metadata":{"execution":{"iopub.status.busy":"2021-09-04T14:31:06.5333Z","iopub.execute_input":"2021-09-04T14:31:06.533683Z","iopub.status.idle":"2021-09-04T14:31:06.539263Z","shell.execute_reply.started":"2021-09-04T14:31:06.533655Z","shell.execute_reply":"2021-09-04T14:31:06.538334Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 2.Exploratory Data Analysis (EDA)\nLet's analyze different features of our datasets to identity abnormalities and their behaviors. We will do the followings:\n- Check the distribution of our target variable, **target**.\n- Investigate the **Location** feature and check its usefulness and possible features we can extract from it.\n- Investigate how helpful the **Keyword** feature is and how that can be used in the model.\n- Go through the **Text** features and identify problems and possible fixes. Also look for possible features that can be extracted and analyzed later on.","metadata":{}},{"cell_type":"markdown","source":"### 2.1.Target feature distrubution","metadata":{}},{"cell_type":"code","source":"# Check the distribution of tagets\ntarget_counts = train_df['target'].value_counts()\n\nfig = plt.figure(figsize = (20, 3))\nplt.title('Target distribution')\nplt.xlabel('Targets')\nplt.ylabel('Counts')\nplt.xticks([0, 1])\nplt.bar(target_counts.keys(), target_counts.values)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-09-04T14:31:06.540353Z","iopub.execute_input":"2021-09-04T14:31:06.540647Z","iopub.status.idle":"2021-09-04T14:31:06.672705Z","shell.execute_reply.started":"2021-09-04T14:31:06.540621Z","shell.execute_reply":"2021-09-04T14:31:06.671735Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Looking at the distribution, I say it's OK. It's not evenly balanced and that can be something we might want to fix by downcasting the data later when optimizing the model. Downcasting may or may not be a good idea as we loose some of our data! (You have to test it to find out its impact on the model)","metadata":{}},{"cell_type":"markdown","source":"### 2.2.Location feature","metadata":{}},{"cell_type":"markdown","source":"#### 2.2.1Analysis","metadata":{}},{"cell_type":"code","source":"# NULL entries\nnull_entries = train_df.loc[train_df['location'].isnull()]\nnull_entries_count = len(null_entries)\nprint(f'- There are {null_entries_count} missing entries on \"location\" column ({round(null_entries_count / train_df.shape[0] * 100, 1)}%)')\n\n# Uniqueness\nunique_locations = train_df['location'].unique()\nunique_locations_count = len(unique_locations)\nprint(f'- Number of unique locations: {unique_locations_count} ({round(unique_locations_count/train_df.shape[0]*100, 2)}%)')","metadata":{"execution":{"iopub.status.busy":"2021-09-04T14:31:06.673867Z","iopub.execute_input":"2021-09-04T14:31:06.674151Z","iopub.status.idle":"2021-09-04T14:31:06.685362Z","shell.execute_reply.started":"2021-09-04T14:31:06.674124Z","shell.execute_reply":"2021-09-04T14:31:06.684257Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 20 Top Frequencies\nlocation_freq = train_df['location'].value_counts()\ntop_20_locs = location_freq[:20]\n\nfig = plt.figure(figsize = (20, 3))\nplt.title('20 Most used Locations')\nplt.bar(top_20_locs.index, top_20_locs.values)\nplt.xticks(rotation = 30)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-09-04T14:31:06.686552Z","iopub.execute_input":"2021-09-04T14:31:06.686864Z","iopub.status.idle":"2021-09-04T14:31:06.943293Z","shell.execute_reply.started":"2021-09-04T14:31:06.686835Z","shell.execute_reply":"2021-09-04T14:31:06.94218Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"After a careful investigation of the **location** feature, I found some interesting facts:\n- About 33% of the entries are missing. Questions: *Why? Are they really missing or ther is a relation to disaster tweets?*\n- About 44% of the entried are unique. Question: *Are tweets focused around spesific areas? Or we just ran out of cities?*\n- There are lots of problems in the entries, for instance:\n    - Some entries only contain numbers, like *304* or *404* (These are familiar if you have done web-dev)\n    - Some entries contain [Geographic coordinate system](https://en.wikipedia.org/wiki/Geographic_coordinate_system).\n    - Some entries contain dates, times and months.\n    - Many entries aren't relevant to location at all. This is because some people write random stuff in their location field. We can use the fact that many locations are not actually locations, feature engineer them into a new binary columns **is_relevant** and check if that gives us a good feature to work with.\n    - Many countries are in [iso code](https://en.wikipedia.org/wiki/List_of_ISO_3166_country_codes), if we are using models like Google's Word2Vec or GloVe, this can be problematic. To fix it I'll use an external dataset.\n    - Some entities contains #hashtags, @mentions and links. We can extract them as well.\n\nSo this is going to be a 2-step process: Cleanup and patchup, then feature Engineering. But first let's write some functions to help us along the way:","metadata":{}},{"cell_type":"markdown","source":"#### 2.2.2.Cleanup, Patchup and Feature Extraction","metadata":{}},{"cell_type":"code","source":"# Remove URLs\ndef remove_urls(text):\n    return re.sub(r'https?://\\S+|www\\.\\S+', '', text)\n\n# Remove HTML\ndef remove_html(text):\n    return re.sub(r'<.*?>', '', text)\n\n# Converts text to lowercase\ndef to_lower(text):\n    return text.lower()\n\n# Remove numbers\ndef remove_numbers(text):\n    return re.sub(r'\\d+', '', text)\n\n# Remove mentions\ndef remove_mentions(text):\n    return re.sub(r'@\\w*', '', text)\n\n# Remove hashtags\ndef remove_hashtags(text):\n    return re.sub(r'#\\w*', '', text)\n\n# Remove links\ndef remove_links(text):\n    return re.sub(r'\\bhttps?://\\S+', '', text)\n\n# Remove emojis\ndef remove_emojis(text):\n    regrex_pattern = re.compile(pattern = \"[\"\n        u\"\\U0001F600-\\U0001F64F\"\n        u\"\\U0001F300-\\U0001F5FF\"\n        u\"\\U0001F680-\\U0001F6FF\"\n        u\"\\U0001F1E0-\\U0001F1FF\"\n    \"]+\", flags = re.UNICODE)\n    return regrex_pattern.sub(r'',text)\n\n# Remove non-ASCII characters\ndef remove_non_ascii(text):\n    return ''.join(filter(lambda x: x in string.printable, text))\n\n# Lemmatize text\ndef lemmatize_text(text):\n    return ' '.join([token.lemma_ for token in nlp(text)])\n\n# Remove stopwords\ndef remove_stopwords(text):\n    return ' '.join([token.text for token in nlp(text) if not token.is_stop])\n\n# Remove punctuation\ndef remove_punctuation(text):\n    table = str.maketrans('', '', string.punctuation)\n    return text.translate(table)\n\n# Remove white space (Extra step, just in case)\ndef remove_whitespace(text):\n    return ' '.join(text.split())","metadata":{"execution":{"iopub.status.busy":"2021-09-04T14:31:06.945361Z","iopub.execute_input":"2021-09-04T14:31:06.945662Z","iopub.status.idle":"2021-09-04T14:31:06.957992Z","shell.execute_reply.started":"2021-09-04T14:31:06.945634Z","shell.execute_reply":"2021-09-04T14:31:06.956924Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Extract hashtags in form of #<str>\ndef extract_hashtags(text):\n    matches = re.findall(r'#\\w*', text)\n    return [match[1:].lower() for match in matches] if matches != [] else np.nan\n\n# Extract mentions in form of @<str>\ndef extract_mentions(text):\n    matches = re.findall(r'@\\w*', text)\n    return [match[1:].lower() for match in matches] if matches != [] else np.nan\n\n# Extract links\ndef extract_links(text):\n    matches = re.findall(r'\\bhttps?://\\S+', text)\n    return matches if matches != [] else np.nan\n\n# Extract cities\ndef extract_city(text):\n    clean_text = remove_punctuation(text)\n    _cities = list([word.capitalize() for word in clean_text.split() if word in cities])\n    return _cities[0] if _cities != [] else np.nan\n\n# Extract countries\ndef extract_country(text):\n    clean_text = remove_punctuation(text)\n    _countries = [countries_df.loc[countries_df['iso2'] == word, 'country'].values[0] for word in clean_text.split() if word in iso2]  \n    return _countries[0] if _countries != [] else np.nan","metadata":{"execution":{"iopub.status.busy":"2021-09-04T14:31:06.959521Z","iopub.execute_input":"2021-09-04T14:31:06.959869Z","iopub.status.idle":"2021-09-04T14:31:06.976647Z","shell.execute_reply.started":"2021-09-04T14:31:06.959838Z","shell.execute_reply":"2021-09-04T14:31:06.975305Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Conditions for train & test data\nloc_not_na = [train_df['location'].notnull(), test_df['location'].notnull()]\n\n\nfor i, df in enumerate(combined):\n    con = loc_not_na[i]\n    \n    # Convert to lower\n    df.loc[con, 'location'] = df.loc[con, 'location'].apply(to_lower)\n\n    # Remove Non-ASCII\n    df.loc[con, 'location'] = df.loc[con, 'location'].apply(remove_non_ascii)\n    \n    # Feature Extract #hashtags, @mentions and links\n    df.loc[con, 'location_hashtags'] = df.loc[con, 'location'].apply(extract_hashtags)\n    df.loc[con, 'location_mentions'] = df.loc[con, 'location'].apply(extract_mentions)\n    df.loc[con, 'location_links'] = df.loc[con, 'location'].apply(extract_links)\n\n    # Feature Extract countries and cities\n    df['city'] = np.nan\n    df['country'] = np.nan\n    df.loc[con, 'city'] = df.loc[con, 'location'].apply(extract_city)\n    df.loc[con, 'country'] = df.loc[con, 'location'].apply(extract_country)\n\n    # Feature Extract\n    df['has_location'] = 0\n    df.loc[(df['city'].notnull()) | (df['country'].notnull()), 'has_location'] = 1","metadata":{"execution":{"iopub.status.busy":"2021-09-04T14:31:06.978221Z","iopub.execute_input":"2021-09-04T14:31:06.97856Z","iopub.status.idle":"2021-09-04T14:31:14.360703Z","shell.execute_reply.started":"2021-09-04T14:31:06.978528Z","shell.execute_reply":"2021-09-04T14:31:14.359599Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"By now, we have some extracted some new features from **location** and what's left, I think is not very useful (We could remove it, but patience):\n- location_hashtags\n- location_mentions\n- location_links\n- country\n- city","metadata":{}},{"cell_type":"markdown","source":"### 2.3.Keyword feature","metadata":{}},{"cell_type":"markdown","source":"#### 2.3.1.Analysis","metadata":{}},{"cell_type":"code","source":"# NULL entries\nnull_entries = train_df.loc[train_df['keyword'].isnull()]\nnull_entries_count = len(null_entries)\nprint(f'- There are {null_entries_count} missing entries on \"keyword\" column ({round(null_entries_count / train_df.shape[0] * 100, 2)}%)')\n\n# Uniqueness\nunique_locations = train_df['keyword'].unique()\nunique_locations_count = len(unique_locations)\nprint(f'- Number of unique keyword: {unique_locations_count} ({round(unique_locations_count/train_df.shape[0]*100, 2)}%)')","metadata":{"execution":{"iopub.status.busy":"2021-09-04T14:31:14.362331Z","iopub.execute_input":"2021-09-04T14:31:14.362769Z","iopub.status.idle":"2021-09-04T14:31:14.379127Z","shell.execute_reply.started":"2021-09-04T14:31:14.362722Z","shell.execute_reply":"2021-09-04T14:31:14.377885Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 20 Top Frequencies\nkeyword_freq = train_df['keyword'].value_counts()\nkeys_25_locs = keyword_freq[:25]\n\nfig = plt.figure(figsize = (20, 3))\nplt.title('25 Most used Keywords')\nplt.bar(keys_25_locs.index, keys_25_locs.values)\nplt.xticks(rotation = 30)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-09-04T14:31:14.380549Z","iopub.execute_input":"2021-09-04T14:31:14.380977Z","iopub.status.idle":"2021-09-04T14:31:14.666212Z","shell.execute_reply.started":"2021-09-04T14:31:14.380942Z","shell.execute_reply":"2021-09-04T14:31:14.665199Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"By analyzing the results and taking a quick look at the data itself, I found:\n- There are only 8 missing entries (less than 1%) which we can take care of by imputing them.\n- Only 222 unique keywords exist (~3%) which might be too good or too bad for us, We'll see..\n- Looking at the 25 most frequently used keywords, they are mostly about disasters (more than 80% of top 25 keywords)\n- Keywords must be lemmatized. (For instance we have both *sinking* and *sunk*)","metadata":{}},{"cell_type":"markdown","source":"#### 2.3.2.Feature Engineering","metadata":{}},{"cell_type":"code","source":"for df in combined:\n    keyword_not_na = df['keyword'].notnull()\n    df.loc[keyword_not_na, 'keyword'] = df.loc[keyword_not_na, 'keyword'].apply(lemmatize_text)","metadata":{"execution":{"iopub.status.busy":"2021-09-04T14:31:14.667377Z","iopub.execute_input":"2021-09-04T14:31:14.667662Z","iopub.status.idle":"2021-09-04T14:32:29.777563Z","shell.execute_reply.started":"2021-09-04T14:31:14.667635Z","shell.execute_reply":"2021-09-04T14:32:29.776514Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 20 Top Frequencies\nkeyword_freq = train_df['keyword'].value_counts()\nkeys_25_locs = keyword_freq[:25]\n\nfig = plt.figure(figsize = (20, 3))\nplt.title('25 Most used Keywords')\nplt.bar(keys_25_locs.index, keys_25_locs.values)\nplt.xticks(rotation = 30)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-09-04T14:32:29.778886Z","iopub.execute_input":"2021-09-04T14:32:29.779182Z","iopub.status.idle":"2021-09-04T14:32:30.064386Z","shell.execute_reply.started":"2021-09-04T14:32:29.779153Z","shell.execute_reply":"2021-09-04T14:32:30.063326Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# train_df.loc[train_df['keyword'] == 'sink'].head(40)\n# train_df.groupby(by = 'is_relevent', as_index = False)['has_location'].count()\n# train_df['has_location'].unique()\n# train_df.loc[(train_df['city'].notnull()) | (train_df['country'].notnull())]","metadata":{"execution":{"iopub.status.busy":"2021-09-04T14:33:00.901994Z","iopub.execute_input":"2021-09-04T14:33:00.902375Z","iopub.status.idle":"2021-09-04T14:33:00.906336Z","shell.execute_reply.started":"2021-09-04T14:33:00.902344Z","shell.execute_reply":"2021-09-04T14:33:00.905429Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 2.4.Text","metadata":{}},{"cell_type":"markdown","source":"For the sake of practice, I am going to extract as much information as I can from the text, this includes extracting links, hashtags and mentions and anything that can be considered useful. Some of the tasks you might want to do:\n- There are lot's of #hashtags used that can be helpful, so extract and store them in a list for each tweet.\n- There also are some links, however, since links are shortened (encoded), they are useless in their basic form. We could scrape each link for headlines and titles, but I'm going to remove them for sipmplicity.\n- @mentions can be seen in some tweets, maybe they point to a certain user or something, extract the username and store it in a list for each tweet for later analysis.\n- There are some emojies and None-ASCII characters that are better be removed.\n- Numbers can also be removed as they may be a little misleading.\n- Some comments are fully written in uppercase, we may want to make them lowercase; Or there are words that don't have to be in upper form, like ALLAH.\n- There are comments that are almost identical except their links, we can easily detect and remove the duplicates after extracting links (See text[110:120])\n- Maybe finding the common words can be useful for identifing which class does each tweet belong to.\n- There are some typos in some texts, they can be fixed with some python packages.\n- We could extract emojis!","metadata":{}},{"cell_type":"code","source":"train_df['text'].iloc[:50].values","metadata":{"execution":{"iopub.status.busy":"2021-08-07T14:31:04.747979Z","iopub.execute_input":"2021-08-07T14:31:04.748277Z","iopub.status.idle":"2021-08-07T14:31:04.755344Z","shell.execute_reply.started":"2021-08-07T14:31:04.74825Z","shell.execute_reply":"2021-08-07T14:31:04.754411Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Feature Extraction","metadata":{}},{"cell_type":"markdown","source":"We extract linkes, hashtags and mentions from the tweet texts and store them  in the dataset as lists. By a more careful analysis, it can be shown that links have been shortened. You could scrape each link and search for topics and healines but since this is time consuming and there aren't many links in the dataset, I decided to use a more simple approach and remove them all.\n\nI also found @mentions useless as they are usernames and usernames can be pretty meaningless and made of random characters. Let's not over-complicate the analysis and remove them as well.\n\nBut #hashtags are still very useful as they are used to tag topics and can be used to group tweets.","metadata":{}},{"cell_type":"code","source":"# Create two new features for #hashtags, @mentions and links\nfor df in combined:\n    df['text_hashtags'] = df['text'].apply(extract_hashtag)\n    df['text_mentions'] = df['text'].apply(extract_mention)\n    df['text_links'] = df['text'].apply(extract_links)","metadata":{"execution":{"iopub.status.busy":"2021-08-07T14:31:04.767714Z","iopub.execute_input":"2021-08-07T14:31:04.768084Z","iopub.status.idle":"2021-08-07T14:31:04.889553Z","shell.execute_reply.started":"2021-08-07T14:31:04.768054Z","shell.execute_reply":"2021-08-07T14:31:04.888659Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"feature_counts = train_df.count()\n\nfig = plt.figure(figsize = (20, 3))\nplt.title('Features counts')\nplt.xlabel('Feature')\nplt.ylabel('Count')\nplt.bar(feature_counts.keys(), feature_counts.values)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-08-07T14:31:04.891038Z","iopub.execute_input":"2021-08-07T14:31:04.891461Z","iopub.status.idle":"2021-08-07T14:31:05.065834Z","shell.execute_reply.started":"2021-08-07T14:31:04.89141Z","shell.execute_reply":"2021-08-07T14:31:05.065088Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As you can see, many hastags, mentions and links are missing already. They aren't much beneficial now, sadly.\nThat brings us to our first place, with no new features to work with. But that's ok because that was necessary to get us to the point where we can be confident in our decisions about removing them from our text data.","metadata":{}},{"cell_type":"markdown","source":"#### Wordclouds","metadata":{}},{"cell_type":"code","source":"# Word cloud for disaster tweets\nplt.figure(figsize = (10, 8))\nword_cloud = WordCloud(max_font_size = 50).generate(\" \".join(train_df.loc[train_df['target'] == 1, 'text'].iloc[:100]))\nplt.imshow(word_cloud)\nplt.axis('off')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-08-07T14:31:05.067085Z","iopub.execute_input":"2021-08-07T14:31:05.067667Z","iopub.status.idle":"2021-08-07T14:31:05.497302Z","shell.execute_reply.started":"2021-08-07T14:31:05.067622Z","shell.execute_reply":"2021-08-07T14:31:05.496292Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Word cloud for non-disaster tweets\nplt.figure(figsize = (10, 8))\nword_cloud = WordCloud(max_font_size = 50).generate(\" \".join(train_df.loc[train_df['target'] == 0, 'text'].iloc[:100]))\nplt.imshow(word_cloud)\nplt.axis('off')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-08-07T14:31:05.498642Z","iopub.execute_input":"2021-08-07T14:31:05.498939Z","iopub.status.idle":"2021-08-07T14:31:05.902623Z","shell.execute_reply.started":"2021-08-07T14:31:05.498909Z","shell.execute_reply":"2021-08-07T14:31:05.901851Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Data Cleaning","metadata":{}},{"cell_type":"markdown","source":"This is a quite important step. After randomly looking at out text data, I found it usefull to do the followings:\n- Remove all links\n- Remove all HTML tags\n- Remove Emojis (Using their unicode)\n- Remove all @mentions\n- Remove all non-ASCII characters\n- Lemmatize all words (This is a very important step, since lemmatization is a way to reduce the number of words in the text)\n- Remove all stop words (Most stop words don't provided much information)\n- Remove all punctuations (Careful with this one, since it might remove important information that can be used in Embeddings later on...)\n- Conver all words to lower case (Danagerous for Embeddings! - \"MIT\" is a university in US, but \"mit\" means \"with\" in germany)\n- Fix text case (Some comments are fully uppercase, it's a good practice to convert them into lowercase)","metadata":{}},{"cell_type":"code","source":"# Remove URLs\ndef remove_urls(text):\n    return re.sub(r'https?://\\S+|www\\.\\S+', '', text)\n\n# Remove HTML\ndef remove_html(text):\n    return re.sub(r'<.*?>', '', text)\n\n# Converts text to lowercase\ndef to_lower(text):\n    return text.lower()\n\n# Remove numbers\ndef remove_numbers(text):\n    return re.sub(r'\\d+', '', text)\n\n# Remove mentions\ndef remove_mentions(text):\n    return re.sub(r'@\\w*', '', text)\n\n# Remove emojis\ndef remove_emojis(text):\n    regrex_pattern = re.compile(pattern = \"[\"\n        u\"\\U0001F600-\\U0001F64F\"\n        u\"\\U0001F300-\\U0001F5FF\"\n        u\"\\U0001F680-\\U0001F6FF\"\n        u\"\\U0001F1E0-\\U0001F1FF\"\n    \"]+\", flags = re.UNICODE)\n    return regrex_pattern.sub(r'',text)\n\n# Remove non-ASCII characters\ndef remove_non_ascii(text):\n    return ''.join(filter(lambda x: x in string.printable, text))\n\n# Lemmatize text\ndef lemmatize_text(text):\n    return ' '.join([token.lemma_ for token in nlp(text)])\n\n# Remove stopwords\ndef remove_stopwords(text):\n    return ' '.join([token.text for token in nlp(text) if not token.is_stop])\n\n# Remove punctuation\ndef remove_punctuation(text):\n    table = str.maketrans('', '', string.punctuation)\n    return text.translate(table)\n\n# Remove white space (Extra step, just in case)\ndef remove_whitespace(text):\n    return ' '.join(text.split())\n\ndef clean_text(text, lower = True,html = True,urls = True, numbers = True, mentions = True, ascii = True,\n               punctuations = True, stopwords = True, lemmatize = True):\n    ''' Properties to remove when cleaning the given text'''    \n\n    # Clean text\n    if lower:\n        text = to_lower(text)\n    if html:\n        text = remove_html(text)\n    if urls:\n        text = remove_urls(text)\n    if numbers:\n        text = remove_numbers(text)\n    if mentions:\n        text = remove_mentions(text)\n    if ascii:\n        text = remove_non_ascii(text)\n    if punctuations:\n        text = remove_punctuation(text)\n    if stopwords:\n        text = remove_stopwords(text)\n    if lemmatize:\n        text = lemmatize_text(text)\n\n    return remove_whitespace(text)      # Last step just in case","metadata":{"execution":{"iopub.status.busy":"2021-08-07T14:31:05.903636Z","iopub.execute_input":"2021-08-07T14:31:05.904017Z","iopub.status.idle":"2021-08-07T14:31:05.918432Z","shell.execute_reply.started":"2021-08-07T14:31:05.903988Z","shell.execute_reply":"2021-08-07T14:31:05.917364Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for df in combined:\n    df['cleaned_text'] = df['text'].apply(clean_text)\n\ntrain_df.head()","metadata":{"execution":{"iopub.status.busy":"2021-08-07T14:31:05.91957Z","iopub.execute_input":"2021-08-07T14:31:05.919898Z","iopub.status.idle":"2021-08-07T14:34:07.532377Z","shell.execute_reply.started":"2021-08-07T14:31:05.919869Z","shell.execute_reply":"2021-08-07T14:34:07.531427Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Deep Learning Model","metadata":{}},{"cell_type":"code","source":"# Drop records with no texts from training data, replace the emoty texts for test data with '?'\ntrain_df.dropna(axis = 0, subset = ['cleaned_text'], inplace = True)\ntest_df['cleaned_text'].fillna('?', inplace = True)\n\n# Shuffle\ntrain_df = train_df.sample(frac = 1).reset_index(drop = True)\n\ntrain_df.info()","metadata":{"execution":{"iopub.status.busy":"2021-08-07T14:34:07.533791Z","iopub.execute_input":"2021-08-07T14:34:07.534349Z","iopub.status.idle":"2021-08-07T14:34:07.564829Z","shell.execute_reply.started":"2021-08-07T14:34:07.534304Z","shell.execute_reply":"2021-08-07T14:34:07.564043Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Encoding","metadata":{}},{"cell_type":"code","source":"tokenizer = Tokenizer(lower = True, oov_token = '?')\ntokenizer.fit_on_texts(train_df['cleaned_text'])\n\n# Modes: 'binary', 'count', 'freq', 'tfidf'\ntrain_encodes = tokenizer.texts_to_matrix(train_df['cleaned_text'], mode = 'count')\ntest_encodes = tokenizer.texts_to_matrix(test_df['cleaned_text'], mode = 'count')\n\nprint('Matrix shape:', train_encodes.shape)","metadata":{"execution":{"iopub.status.busy":"2021-08-07T14:34:07.565923Z","iopub.execute_input":"2021-08-07T14:34:07.566378Z","iopub.status.idle":"2021-08-07T14:34:08.05991Z","shell.execute_reply.started":"2021-08-07T14:34:07.566333Z","shell.execute_reply":"2021-08-07T14:34:08.058961Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Building the Model","metadata":{}},{"cell_type":"code","source":"def build_model(x, loss, optimizer, metrics):\n    model = Sequential()\n    model.add(Dense(units = 64, input_shape = (x.shape[1],), activation = 'relu'))\n    model.add(Dense(units = 1, activation = 'sigmoid'))\n    model.compile(loss = loss, optimizer = optimizer, metrics = metrics)  \n\n    return model","metadata":{"execution":{"iopub.status.busy":"2021-08-07T14:34:08.061205Z","iopub.execute_input":"2021-08-07T14:34:08.061536Z","iopub.status.idle":"2021-08-07T14:34:08.067148Z","shell.execute_reply.started":"2021-08-07T14:34:08.061486Z","shell.execute_reply":"2021-08-07T14:34:08.066152Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Training the Model","metadata":{}},{"cell_type":"code","source":"def train_model(model, x, y, epochs, batch_size, validation_split):\n    history = model.fit(\n    x = x,\n    y = y,\n    epochs = epochs,\n    batch_size = batch_size,\n    validation_split = validation_split\n    )\n\n    return history.history","metadata":{"execution":{"iopub.status.busy":"2021-08-07T14:34:08.068287Z","iopub.execute_input":"2021-08-07T14:34:08.068597Z","iopub.status.idle":"2021-08-07T14:34:08.080643Z","shell.execute_reply.started":"2021-08-07T14:34:08.068566Z","shell.execute_reply":"2021-08-07T14:34:08.07981Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Plotting the Model","metadata":{}},{"cell_type":"code","source":"def plot_model_training(epochs, history, metrics):    \n\n    def plot_subplot(axs, metric, val_metric):\n        ''' Plot a single subplot '''\n\n        axs.set_title('Analysis of ' + metric)\n        axs.plot(epochs, history[metric], label = metric)\n        axs.plot(epochs, history[val_metric], label = val_metric)\n        axs.legend()\n\n    fig, axs = plt.subplots(1, len(metrics), figsize = (18, 5))\n\n    for i, metric in enumerate(metrics):\n        plot_subplot(axs[i], metric, 'val_' + metric)","metadata":{"execution":{"iopub.status.busy":"2021-08-07T14:34:08.081867Z","iopub.execute_input":"2021-08-07T14:34:08.082314Z","iopub.status.idle":"2021-08-07T14:34:08.092337Z","shell.execute_reply.started":"2021-08-07T14:34:08.082269Z","shell.execute_reply":"2021-08-07T14:34:08.091537Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### All in One","metadata":{}},{"cell_type":"code","source":"X_train = train_encodes\ny_train = train_df['target']\n\n# Build model\nmodel = build_model(\n  x = X_train,\n  loss = 'binary_crossentropy',\n  optimizer = 'adam',\n  metrics = ['accuracy']\n)\n\n# Plot model\nplot_model(model = model, show_dtype = True, show_shapes = True, show_layer_names = False)\n\n# Train model\nhistory = train_model(\n  model = model,\n  x = X_train,\n  y = y_train,\n  epochs = 1,\n  batch_size = 64,\n  validation_split = 0.2\n)\n\n# Plot training process\nepochs = [i for i in range(len(history['loss']))]\nplot_model_training(\n  epochs = epochs,\n  history = history,\n  metrics = ['accuracy', 'loss']\n)","metadata":{"execution":{"iopub.status.busy":"2021-08-07T14:34:08.093627Z","iopub.execute_input":"2021-08-07T14:34:08.094132Z","iopub.status.idle":"2021-08-07T14:34:11.884344Z","shell.execute_reply.started":"2021-08-07T14:34:08.094096Z","shell.execute_reply":"2021-08-07T14:34:11.883221Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Make a Prediction","metadata":{}},{"cell_type":"code","source":"X_test = test_encodes\n\npred = model.predict(X_test, verbose = 2)\npred = np.round(pred).astype(int).reshape(pred.shape[0])\n\n# Create submission\nsubmission = pd.DataFrame({'id': test_df['id'].values.tolist(), 'target': pred})\nsubmission.to_csv('./submission.csv', index = False)","metadata":{"execution":{"iopub.status.busy":"2021-08-07T14:34:11.887794Z","iopub.execute_input":"2021-08-07T14:34:11.888115Z","iopub.status.idle":"2021-08-07T14:34:12.436678Z","shell.execute_reply.started":"2021-08-07T14:34:11.888084Z","shell.execute_reply":"2021-08-07T14:34:12.43575Z"},"trusted":true},"execution_count":null,"outputs":[]}]}