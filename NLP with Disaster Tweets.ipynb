{"cells":[{"cell_type":"markdown","metadata":{"id":"6H0ziAsdQfah"},"source":["# NLP with Disaster Tweets"]},{"cell_type":"markdown","metadata":{},"source":["## 0.Sync with Google Drive"]},{"cell_type":"markdown","metadata":{"id":"w5Ma28OXQfa5"},"source":["## 1.Load Data \u0026 Libraries"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"L8kjhfcUQfa5"},"outputs":[],"source":["import re                                                 # Regex for string extraction\r\n","import spacy                                              # Spacy for text processing\r\n","import string                                             # For using string functions and properties\r\n","import numpy as np                                        # Linear Algebra\r\n","import pandas as pd                                       # Pandas for DataFrame\r\n","import seaborn as sns                                     # Sns for plots\r\n","from scipy import stats\r\n","from matplotlib import pyplot as plt                      # Matplotlib for plots\r\n","from wordcloud import WordCloud                           # Wordclouds\r\n","\r\n","# Keras packages\r\n","from keras.preprocessing.text import Tokenizer            # Simplifies different tokenizing methods\r\n","from keras.utils.vis_utils import plot_model              # For plotting model\r\n","from keras.models import Sequential                       # Sequential model\r\n","from keras.layers import Dense                            # Dense layer\r\n","\r\n","nlp = spacy.load('en_core_web_sm')  # Load the English language (Other options: en_core_web_md, en_core_web_lg)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"pydPF-5xQfa5"},"outputs":[{"ename":"NameError","evalue":"ignored","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m\u003cipython-input-2-f41f9cf356fc\u003e\u001b[0m in \u001b[0;36m\u003cmodule\u003e\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Load original data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----\u003e 2\u001b[0;31m \u001b[0mtrain_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'{repo_dir}/Data/Raw/train.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mtest_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'{repo_dir}/Data/Raw/test.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Combine original data for easier manipulating them later\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'repo_dir' is not defined"]}],"source":["# Load original data\r\n","train_df = pd.read_csv(f'{repo_dir}/Data/Raw/train.csv')\r\n","test_df = pd.read_csv(f'{repo_dir}/Data/Raw/test.csv')\r\n","\r\n","# Combine original data for easier manipulating them later\r\n","combined = [train_df, test_df]"]},{"cell_type":"markdown","metadata":{"id":"ajkJM1EiQfa6"},"source":["I'll also load an external dataset to help speedup city and country extraction:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8EDlOWFoQfa6"},"outputs":[],"source":["# Load cities data\r\n","cities_df = pd.read_csv(f'{repo_dir}/Data/External/worldcities.csv')\r\n","cities_df['city'] = cities_df['city'].apply(lambda x: x.lower())\r\n","\r\n","# Get a list of cities\r\n","cities = list(cities_df['city'])\r\n","\r\n","# Create country_df\r\n","countries_df = cities_df[['iso2', 'iso3', 'country']].groupby(by = 'country', as_index = False).max()\r\n","countries_df.dropna(axis = 0, inplace = True)                                 # Drop NAs\r\n","countries_df['iso2'] = countries_df['iso2'].apply(lambda x: x.lower())\r\n","iso2 = list(countries_df['iso2'])"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":12,"status":"ok","timestamp":1631042601783,"user":{"displayName":"Keivan Ipchi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiYWJZ-pareZcENvSntLEr0hS7wN6xv1TYBeXeu=s64","userId":"05399847325073645104"},"user_tz":-270},"id":"M03d9IOYQfa6"},"outputs":[],"source":["# Print basic properties\r\n","print(f\"Training Data has {train_df.shape[0]} rows and {train_df.shape[1]} columns: ({list(train_df.columns)})\")\r\n","print(f\"Testing Data has {test_df.shape[0]} rows and {test_df.shape[1]} columns: ({list(test_df.columns)})\")"]},{"cell_type":"markdown","metadata":{"id":"qiOFwQQHW6tF"},"source":["## 2.Exploratory Data Analysis (EDA)"]},{"cell_type":"markdown","metadata":{"id":"KjCtSDM-Qfa6"},"source":["Let's analyze different features of our datasets to identity abnormalities and their behaviors. We will do the followings:\n","- Check the distribution of our target variable, **target**.\n","- Investigate the **Location** feature and check its usefulness and possible features we can extract from it.\n","- Investigate how helpful the **Keyword** feature is and how that can be used in the model.\n","- Go through the **Text** features and identify problems and possible fixes. Also look for possible features that can be extracted and analyzed later on."]},{"cell_type":"markdown","metadata":{"id":"6cwzju9hQfa6"},"source":["### 2.1.Target feature distrubution"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":886,"status":"ok","timestamp":1631042602658,"user":{"displayName":"Keivan Ipchi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiYWJZ-pareZcENvSntLEr0hS7wN6xv1TYBeXeu=s64","userId":"05399847325073645104"},"user_tz":-270},"id":"rZaBYpY_Qfa7"},"outputs":[],"source":["# Check the distribution of tagets\r\n","target_counts = train_df['target'].value_counts()\r\n","\r\n","fig = plt.figure(figsize = (20, 3))\r\n","plt.title('Target distribution')\r\n","plt.xlabel('Targets')\r\n","plt.ylabel('Counts')\r\n","plt.xticks([0, 1])\r\n","plt.bar(target_counts.keys(), target_counts.values)\r\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"QAtvWYjhQfa7"},"source":["Looking at the distribution, I say it's OK. It's not evenly balanced and that can be something we might want to fix by downcasting the data later when optimizing the model. Downcasting may or may not be a good idea as we loose some of our data! (You have to test it to find out its impact on the model)"]},{"cell_type":"markdown","metadata":{"id":"w_ueJpbOQfa7"},"source":["### 2.2.Location feature"]},{"cell_type":"markdown","metadata":{"id":"TVvkipILQfa7"},"source":["#### 2.2.1Analysis"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":11,"status":"ok","timestamp":1631042602664,"user":{"displayName":"Keivan Ipchi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiYWJZ-pareZcENvSntLEr0hS7wN6xv1TYBeXeu=s64","userId":"05399847325073645104"},"user_tz":-270},"id":"D4jyyX8cQfa7"},"outputs":[],"source":["# NULL entries\r\n","null_entries = train_df.loc[train_df['location'].isnull()]\r\n","null_entries_count = len(null_entries)\r\n","print(f'- There are {null_entries_count} missing entries on \"location\" column ({round(null_entries_count / train_df.shape[0] * 100, 1)}%)')\r\n","\r\n","# Uniqueness\r\n","unique_locations = train_df['location'].unique()\r\n","unique_locations_count = len(unique_locations)\r\n","print(f'- Number of unique locations: {unique_locations_count} ({round(unique_locations_count/train_df.shape[0]*100, 2)}%)')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":12,"status":"ok","timestamp":1631042602666,"user":{"displayName":"Keivan Ipchi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiYWJZ-pareZcENvSntLEr0hS7wN6xv1TYBeXeu=s64","userId":"05399847325073645104"},"user_tz":-270},"id":"NGiTuKCuQfa7"},"outputs":[],"source":["# 20 Top Frequencies\r\n","location_freq = train_df['location'].value_counts()\r\n","top_20_locs = location_freq[:20]\r\n","\r\n","fig = plt.figure(figsize = (20, 3))\r\n","plt.title('20 Most used Locations')\r\n","plt.bar(top_20_locs.index, top_20_locs.values)\r\n","plt.xticks(rotation = 30)\r\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"OqtRuOp8Qfa8"},"source":["After a careful investigation of the **location** feature, I found some interesting facts:\n","- About 33% of the entries are missing. Questions: *Why? Are they really missing or ther is a relation to disaster tweets?*\n","- About 44% of the entried are unique. Question: *Are tweets focused around spesific areas? Or we just ran out of cities?*\n","- There are lots of problems in the entries, for instance:\n","    - Some entries only contain numbers, like *304* or *404* (These are familiar if you have done web-dev)\n","    - Some entries contain [Geographic coordinate system](https://en.wikipedia.org/wiki/Geographic_coordinate_system).\n","    - Some entries contain dates, times and months.\n","    - Many entries aren't relevant to location at all. This is because some people write random stuff in their location field. We can use the fact that many locations are not actually locations, feature engineer them into a new binary columns **is_relevant** and check if that gives us a good feature to work with.\n","    - Many countries are in [iso code](https://en.wikipedia.org/wiki/List_of_ISO_3166_country_codes), if we are using models like Google's Word2Vec or GloVe, this can be problematic. To fix it I'll use an external dataset.\n","    - Some entities contains *#hashtags*, *@mentions* and *:links*. We can extract them as well.\n","\n","So this is going to be a 2-step process: Cleanup and patchup, then feature Engineering. But first let's write some functions to help us along the way:"]},{"cell_type":"markdown","metadata":{"id":"m1t0P7RtQfa8"},"source":["#### 2.2.2.Cleanup, Patchup and Feature Extraction"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"a0c-4LIfQfa8"},"outputs":[],"source":["# Remove URLs\r\n","def remove_urls(text):\r\n","    return re.sub(r'https?://\\S+|www\\.\\S+', '', text)\r\n","\r\n","# Remove HTML\r\n","def remove_html(text):\r\n","    return re.sub(r'\u003c.*?\u003e', '', text)\r\n","\r\n","# Converts text to lowercase\r\n","def to_lower(text):\r\n","    return text.lower()\r\n","\r\n","# Remove numbers\r\n","def remove_numbers(text):\r\n","    return re.sub(r'\\d+', '', text)\r\n","\r\n","# Remove mentions\r\n","def remove_mentions(text):\r\n","    return re.sub(r'@\\w*', '', text)\r\n","\r\n","# Remove hashtags\r\n","def remove_hashtags(text):\r\n","    return re.sub(r'#\\w*', '', text)\r\n","\r\n","# Remove links\r\n","def remove_links(text):\r\n","    return re.sub(r'\\bhttps?://\\S+', '', text)\r\n","\r\n","# Remove emojis\r\n","def remove_emojis(text):\r\n","    regrex_pattern = re.compile(pattern = \"[\"\r\n","        u\"\\U0001F600-\\U0001F64F\"\r\n","        u\"\\U0001F300-\\U0001F5FF\"\r\n","        u\"\\U0001F680-\\U0001F6FF\"\r\n","        u\"\\U0001F1E0-\\U0001F1FF\"\r\n","    \"]+\", flags = re.UNICODE)\r\n","    return regrex_pattern.sub(r'',text)\r\n","\r\n","# Remove non-ASCII characters\r\n","def remove_non_ascii(text):\r\n","#     return ''.join(filter(lambda x: x in string.printable, text))\r\n","    return text.encode(\"ascii\",errors=\"ignore\").decode()\r\n","\r\n","# Lemmatize text\r\n","def lemmatize_text(text):\r\n","    return ' '.join([token.lemma_ for token in nlp(text)])\r\n","\r\n","# Remove stopwords\r\n","def remove_stopwords(text):\r\n","    return ' '.join([token.text for token in nlp(text) if not token.is_stop])\r\n","\r\n","# Remove punctuation\r\n","def remove_punctuation(text):\r\n","    table = str.maketrans('', '', string.punctuation)\r\n","    return text.translate(table)\r\n","\r\n","def convert_spacing_encode(text):\r\n","    return text.replace('%20', ' ')\r\n","\r\n","# Remove white space (Extra step, just in case)\r\n","def remove_whitespace(text):\r\n","    return ' '.join(text.split())\r\n","\r\n","# Filter out words with too few characters (2 by default)\r\n","def filter_words(text):\r\n","    return ' '.join([word for word in text.split() if len(word) \u003e 2])"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"ALSfubB1Qfa8"},"outputs":[],"source":["# Extract hashtags in form of #\u003cstr\u003e (They must have atleast 3 characters)\r\n","def extract_hashtags(text):\r\n","    matches = re.findall(r'#\\w*', text)\r\n","    return [match[1:].lower() for match in matches if len(match) \u003e 2] if matches != [] else np.nan\r\n","\r\n","# Extract mentions in form of @\u003cstr\u003e (They must have atleast 3 characters)\r\n","def extract_mentions(text):\r\n","    matches = re.findall(r'@\\w*', text)\r\n","    return [match[1:].lower() for match in matches if len(match) \u003e 2] if matches != [] else np.nan\r\n","\r\n","# Extract links\r\n","def extract_links(text):\r\n","    matches = re.findall(r'\\bhttps?://\\S+', text)\r\n","    return matches if matches != [] else np.nan\r\n","\r\n","# Extract cities\r\n","def extract_city(text):\r\n","    clean_text = remove_punctuation(text)\r\n","    _cities = list([word.capitalize() for word in clean_text.split() if word in cities])\r\n","    return _cities[0] if _cities != [] else np.nan\r\n","\r\n","# Extract countries\r\n","def extract_country(text):\r\n","    clean_text = remove_punctuation(text)\r\n","    _countries = [countries_df.loc[countries_df['iso2'] == word, 'country'].values[0] for word in clean_text.split() if word in iso2]  \r\n","    return _countries[0] if _countries != [] else np.nan"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"dSNQrdSpQfa9"},"outputs":[],"source":["# Conditions for train \u0026 test data\r\n","loc_not_na = [train_df['location'].notnull(), test_df['location'].notnull()]\r\n","\r\n","\r\n","for i, df in enumerate(combined):\r\n","    con = loc_not_na[i]\r\n","    \r\n","    # Convert to lower\r\n","    df.loc[con, 'location'] = df.loc[con, 'location'].apply(to_lower)\r\n","\r\n","    # Remove Non-ASCII\r\n","    df.loc[con, 'location'] = df.loc[con, 'location'].apply(remove_non_ascii)\r\n","    \r\n","    # Feature Extract #hashtags, @mentions and :links\r\n","    df.loc[con, 'location_hashtags'] = df.loc[con, 'location'].apply(extract_hashtags)\r\n","    df.loc[con, 'location_mentions'] = df.loc[con, 'location'].apply(extract_mentions)\r\n","    df.loc[con, 'location_links'] = df.loc[con, 'location'].apply(extract_links)\r\n","\r\n","    # Feature Extract countries and cities\r\n","    df['city'] = np.nan\r\n","    df['country'] = np.nan\r\n","    df.loc[con, 'city'] = df.loc[con, 'location'].apply(extract_city)\r\n","    df.loc[con, 'country'] = df.loc[con, 'location'].apply(extract_country)\r\n","\r\n","    # Feature Extract\r\n","    df['has_location'] = 0\r\n","    df.loc[(df['city'].notnull()) | (df['country'].notnull()), 'has_location'] = 1"]},{"cell_type":"markdown","metadata":{"id":"w8c2EWhOQfa9"},"source":["By now, we have some extracted some new features from **location** and what's left, I think is not very useful (We could remove it, but patience):\n","- location_hashtags\n","- location_mentions\n","- location_links\n","- country\n","- city"]},{"cell_type":"markdown","metadata":{"id":"Oje0q_hjQfa9"},"source":["### 2.3.Keyword feature"]},{"cell_type":"markdown","metadata":{"id":"YljoV5DQQfa-"},"source":["#### 2.3.1.Analysis"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6,"status":"ok","timestamp":1631042611978,"user":{"displayName":"Keivan Ipchi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiYWJZ-pareZcENvSntLEr0hS7wN6xv1TYBeXeu=s64","userId":"05399847325073645104"},"user_tz":-270},"id":"P73qT4YDQfa-"},"outputs":[],"source":["# NULL entries\r\n","null_entries = train_df.loc[train_df['keyword'].isnull()]\r\n","null_entries_count = len(null_entries)\r\n","print(f'- There are {null_entries_count} missing entries on \"keyword\" column ({round(null_entries_count / train_df.shape[0] * 100, 2)}%)')\r\n","\r\n","# Uniqueness\r\n","unique_locations = train_df['keyword'].unique()\r\n","unique_locations_count = len(unique_locations)\r\n","print(f'- Number of unique keyword: {unique_locations_count} ({round(unique_locations_count/train_df.shape[0]*100, 2)}%)')"]},{"cell_type":"markdown","metadata":{"id":"b1oJdwsSQfa-"},"source":["By analyzing the results and taking a quick look at the data itself, I found:\n","- There are only 8 missing entries (less than 1%) which we can take care of by imputing them.\n","- Only 222 unique keywords exist (~3%) which might be too good or too bad for us, We'll see..\n","- Looking at the 25 most frequently used keywords, they are mostly about disasters (more than 80% of top 25 keywords)\n","- Keywords must be lemmatized. (For instance we have both *sinking* and *sunk*)"]},{"cell_type":"markdown","metadata":{"id":"MXjMMX8LQfa-"},"source":["#### 2.3.2.Feature Engineering"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"yHHanQ3GQfa-"},"outputs":[],"source":["for df in combined:\r\n","    keyword_not_na = df['keyword'].notnull()\r\n","    \r\n","    # Lemmatize\r\n","    df.loc[keyword_not_na, 'keyword'] = df.loc[keyword_not_na, 'keyword'].apply(lemmatize_text)\r\n","    \r\n","    # Fix space encoding    \r\n","    df.loc[keyword_not_na, 'keyword'] = df.loc[keyword_not_na, 'keyword'].apply(convert_spacing_encode)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1197,"status":"ok","timestamp":1631042709329,"user":{"displayName":"Keivan Ipchi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiYWJZ-pareZcENvSntLEr0hS7wN6xv1TYBeXeu=s64","userId":"05399847325073645104"},"user_tz":-270},"id":"bYCJWvieQfa-"},"outputs":[],"source":["fig, axs = plt.subplots(nrows = 3, ncols = 1, figsize = (20, 10))\r\n","fig.tight_layout()\r\n","\r\n","# All\r\n","keyword_freq = train_df['keyword'].value_counts()\r\n","axs[0].set_ylabel('All')\r\n","axs[0].bar(keyword_freq[:20].index, keyword_freq[:20].values)\r\n","\r\n","# target == 1\r\n","keyword_freq = train_df.loc[train_df['target'] == 1, 'keyword'].value_counts()\r\n","axs[1].set_ylabel('Disaster Tweets')\r\n","axs[1].bar(keyword_freq[:20].index, keyword_freq[:20].values)\r\n","\r\n","# target == 0\r\n","keyword_freq = train_df.loc[train_df['target'] == 0, 'keyword'].value_counts()\r\n","axs[2].set_ylabel('Non-Disaster Tweets')\r\n","axs[2].bar(keyword_freq[:20].index, keyword_freq[:20].values)\r\n","\r\n","# Rotate axis\r\n","for ax in axs:\r\n","    plt.sca(ax)\r\n","    plt.xticks(rotation = 10)\r\n","\r\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"MwaJmRWQQfa_"},"source":["#### 2.3.3.More Analysis"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"b2eLbfceQfa_"},"outputs":[],"source":["# Get unique keywords\r\n","keywords = train_df.loc[train_df['keyword'].notnull(), 'keyword'].unique()\r\n","\r\n","# Create keywords dataframe\r\n","keywords_df = pd.DataFrame(data = {'keyword': keywords})\r\n","\r\n","disaster_cnts, total_cnts = [], []\r\n","disaster_probs = []\r\n","for i, row in keywords_df.iterrows():\r\n","\r\n","    tmp_df = train_df.loc[(train_df['keyword'] == row['keyword'])]\r\n","    tmp1_df = tmp_df.loc[tmp_df['target'] == 1]\r\n","    \r\n","    # Save counts\r\n","    disaster_cnts.append(tmp1_df.shape[0])\r\n","    total_cnts.append(tmp_df.shape[0])\r\n","    \r\n","    # Save probabilities\r\n","    disaster_probs.append(tmp1_df.shape[0] / tmp_df.shape[0])\r\n","\r\n","# Add data for disaster tweets\r\n","keywords_df['Disaster_Cnts'] = disaster_cnts\r\n","keywords_df['Total_Cnts'] = total_cnts\r\n","keywords_df['Disaster_Prob'] = disaster_probs"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":37,"status":"ok","timestamp":1631042710293,"user":{"displayName":"Keivan Ipchi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiYWJZ-pareZcENvSntLEr0hS7wN6xv1TYBeXeu=s64","userId":"05399847325073645104"},"user_tz":-270},"id":"p_GqeMJYQfa_"},"outputs":[],"source":["# Get probability for keywords associated with disaster tweets\r\n","keywords_df.sort_values(\r\n","    by = 'Disaster_Prob',\r\n","    ascending = False\r\n",").head(10)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":34,"status":"ok","timestamp":1631042710294,"user":{"displayName":"Keivan Ipchi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiYWJZ-pareZcENvSntLEr0hS7wN6xv1TYBeXeu=s64","userId":"05399847325073645104"},"user_tz":-270},"id":"Cz84Ef18Qfa_"},"outputs":[],"source":["# Get probability for keywords associated with non-disaster tweets\r\n","keywords_df.sort_values(\r\n","    by = 'Disaster_Prob',\r\n",").head(10)"]},{"cell_type":"markdown","metadata":{"id":"w5c_XlptQfa_"},"source":["After looking at a lot of data for the feature, I found problems to be solved and features to be extracted, namely:\n","- Extract *#hashtags* and store then as **text_hashtags**.\n","- Extract *@mentions* and store then as **text_mentions**.\n","- Extract *:links* and store then as **text_links**.\n","- Finally remove *:links* and *@mentions* from the text itself, but keep *#hashtags*?\n","- Remove emojies and None-ASCII characters.\n","- Links can be quite useful as they can be a source of extra information. We might want to scrape each link for possible headlines.\n","- Number of words and characters can be useful in the analysis. I'll extract them as two new features for now.\n"]},{"cell_type":"markdown","metadata":{"id":"Z4byYE9CbclS"},"source":["### 2.4.Text feature"]},{"cell_type":"markdown","metadata":{"id":"k8AKTgGVQfa_"},"source":["#### 2.4.1.Feature Extraction"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"Kn0EzUGiQfa_"},"outputs":[],"source":["for df in combined:\r\n","    \r\n","    # Feature extract #hashtags, @mentions and :links\r\n","    df['text_hashtags'] = df['text'].apply(extract_hashtags)\r\n","    df['text_mentions'] = df['text'].apply(extract_mentions)\r\n","    df['text_links'] = df['text'].apply(extract_links)\r\n","\r\n","    # Remove #hashtagsh, @mentions and :links\r\n","    df['text'] = df['text'].apply(remove_hashtags)\r\n","    df['text'] = df['text'].apply(remove_mentions)\r\n","    df['text'] = df['text'].apply(remove_links)\r\n","    \r\n","    # Feature extract number of words and characters\r\n","    df['characters'] = df['text'].apply(lambda x: len(x))\r\n","    df['words'] = df['text'].apply(lambda x: len(x.split()))"]},{"cell_type":"markdown","metadata":{"id":"JFglfCFnQfa_"},"source":["#### 2.4.2.Analysis"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":32,"status":"ok","timestamp":1631042710295,"user":{"displayName":"Keivan Ipchi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiYWJZ-pareZcENvSntLEr0hS7wN6xv1TYBeXeu=s64","userId":"05399847325073645104"},"user_tz":-270},"id":"HfD3k8e2QfbA"},"outputs":[],"source":["# Get counts\n","train_feature_counts = train_df.count()\n","test_feature_counts = test_df.count()\n","\n","fig, axs = plt.subplots(nrows = 2, ncols = 1, figsize = (20, 7))\n","\n","# train_df\n","axs[0].set_ylabel('train_df Count')\n","axs[0].bar(train_feature_counts.keys(), train_feature_counts.values)\n","\n","# test_df\n","axs[1].set_ylabel('test_df Count')\n","axs[1].bar(test_feature_counts.keys(), test_feature_counts.values)\n","\n","# Rotate axis\n","for ax in axs:\n","    plt.sca(ax)\n","    plt.xticks(rotation = 10)\n","\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3593,"status":"ok","timestamp":1631042713858,"user":{"displayName":"Keivan Ipchi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiYWJZ-pareZcENvSntLEr0hS7wN6xv1TYBeXeu=s64","userId":"05399847325073645104"},"user_tz":-270},"id":"Hw8vMHgAQfbA"},"outputs":[],"source":["fig, axs = plt.subplots(nrows = 1, ncols = 2, figsize = (20, 7))\n","\n","# Disaster Tweets\n","word_cloud = WordCloud(max_font_size = 50)\n","word_cloud.generate(\" \".join(train_df.loc[train_df['target'] == 1, 'text']))\n","axs[0].imshow(word_cloud)\n","axs[0].set_title(\"Disaster Tweets\")\n","axs[0].axis('off')\n","\n","# Normal Tweets\n","word_cloud = WordCloud(max_font_size = 50)\n","word_cloud.generate(\" \".join(train_df.loc[train_df['target'] == 0, 'text']))\n","axs[1].set_title(\"Non-Disaster Tweets\")\n","axs[1].imshow(word_cloud)\n","axs[1].axis('off')\n","\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":31,"status":"ok","timestamp":1631042713859,"user":{"displayName":"Keivan Ipchi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiYWJZ-pareZcENvSntLEr0hS7wN6xv1TYBeXeu=s64","userId":"05399847325073645104"},"user_tz":-270},"id":"_KEggj_NQfbA"},"outputs":[],"source":["# Analyze the numeber of characters\n","plt.hist(train_df.loc[train_df['target'] == 0, 'characters'], label = \"Non-Disaster\", alpha = 0.6)\n","plt.hist(train_df.loc[train_df['target'] == 1, 'characters'], label = \"Disaster\", alpha = 0.6)\n","\n","plt.xlabel(\"Characters\")\n","plt.ylabel(\"Frequency\")\n","plt.title('Characters counts of Tweets')\n","plt.legend()\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":27,"status":"ok","timestamp":1631042713859,"user":{"displayName":"Keivan Ipchi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiYWJZ-pareZcENvSntLEr0hS7wN6xv1TYBeXeu=s64","userId":"05399847325073645104"},"user_tz":-270},"id":"DQlNs1fMQfbA"},"outputs":[],"source":["# Analyze the numeber of words\n","plt.hist(train_df.loc[train_df['target'] == 0, 'words'], label = \"Non-Disaster\", alpha = 0.6)\n","plt.hist(train_df.loc[train_df['target'] == 1, 'words'], label = \"Disaster\", alpha = 0.6)\n","\n","plt.xlabel(\"Words\")\n","plt.ylabel(\"Frequency\")\n","plt.title('Words counts of Tweets')\n","plt.legend()\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"dMU9ITJNQfbA"},"source":["#### 2.4.3.Text Engineering (Pre-processing)\n","An important step is to patchup the **text** feature. There are a lot of pre-processings we could do; However, not all guarantee a better performance. For instance converting all items to lower-case might not be very good for GloVe. There are many simular cases. However, I'm going to do the followings:\n","\n","1. Remove all non-ASCII characters.\n","2. Remove all HTML tags.\n","3. Remove all numbers.\n","4. Remove all emojies.\n","5. Convert all words to lowercase. (We will loose some textual features and gain some other, depends on the model and what you are trying to achive)\n","6. Remove all stopwords. (Stopwords don't provide much information)\n","7. Remove all punctuations.\n","8. Lemmatize all words. (This helps reduce size of the model) (Must be one of the last steps since it takes time to complete)\n","9. Remove words with too few characters (Default: 2)\n","\n","Experiments:\n","- Re-add *#hashtags* back to the text or not remove them in the first place.\n","- Scrape links to gain extra information about the dataset."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":238515,"status":"ok","timestamp":1631042952350,"user":{"displayName":"Keivan Ipchi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiYWJZ-pareZcENvSntLEr0hS7wN6xv1TYBeXeu=s64","userId":"05399847325073645104"},"user_tz":-270},"id":"3DhdYv0GQfbA"},"outputs":[],"source":["for i, df in enumerate(combined):\n","    \n","    text = df['text']\n","\n","    # Remove non-ASCII characters\n","    text = text.apply(remove_non_ascii)\n","    \n","    # Remove HTML tags\n","    text = text.apply(remove_html)\n","    \n","    # Remove numbers\n","    text = text.apply(remove_numbers)    \n","    \n","    # Remove Emojies\n","    text = text.apply(remove_emojis)\n","    \n","    # Convert to lowercase\n","    text = text.apply(to_lower)\n","    \n","    # Remove stopwords\n","    text = text.apply(remove_stopwords)\n","    \n","    # Remove punctuations\n","    text = text.apply(remove_punctuation)\n","\n","    # Lemmatize text\n","    text = text.apply(lemmatize_text)\n","\n","    # Remove words with few characters    \n","    text = text.apply(filter_words)\n","\n","    df['clean_text'] = text\n","\n","    # Log\n","    print(f'{i + 1}th dataset pre-processing complete.')"]},{"cell_type":"markdown","metadata":{"id":"5q0IpPyPW9-G"},"source":["## 3.Feature Selection"]},{"cell_type":"markdown","metadata":{"id":"WqH-5d89QfbA"},"source":["- *@mentions* don't provide much information as they are simply usernames (which can be literally anything), so I'lll drop them.\n","- *:links* can be quite useful, but for **location** we have less than 5 total, so let's not risk and remove **location_links** as well.\n","- Since I don't like the **location** feature (I don't know how it's made and where it comes from?) I don't trust the *#hashtags* it gave us and I'm going to risk dropping them.\n","- Now that we have extracted as much information as we could from **location**, it's no use for us and it's better be removed.\n","- We don't have any missing data, but JIC, I'm going to fill the imaginary NAs with \"?\"\n","- I'm going to seperate the dataframes into two. It can be a potentially good idea to make two models and ensemble them for the final prediction:\n","    - text_df: Contains all the text-related features (**text**, **clean_text**).\n","    - info_df: Contains all other relevant features."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"Oe7jwA5-QfbB"},"outputs":[],"source":["for df in combined:\n","    \n","    # Drop features\n","    df.drop(columns = ['text_mentions', 'location_mentions', 'location_links', 'location', 'location_hashtags'], inplace = True)\n","    \n","    # Fill NAs\n","    df['clean_text'].fillna('?', inplace = True)\n","    \n","    # Shuffle\n","    df = df.sample(frac = 1).reset_index(drop = True)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"JsLZm6qVQfbB"},"outputs":[],"source":["# Create text_df\n","train_text_df = train_df[['id', 'text', 'clean_text', 'target']].copy()\n","test_text_df = test_df[['id', 'text', 'clean_text']].copy()\n","\n","# Create info_df\n","train_info_df = train_df[['id', 'keyword', 'city', 'country', 'has_location', 'text_hashtags', 'text_links', 'characters', 'words', 'target']].copy()\n","test_info_df = test_df[['id', 'keyword', 'city', 'country', 'has_location', 'text_hashtags', 'text_links', 'characters', 'words']].copy()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"YpLJ4NUrrtqR"},"outputs":[],"source":["# Save datasets for later modeling\n","train_text_df.to_csv(f'{repo_dir}/Data/Processed/train_text_df.csv', index = False)\n","test_text_df.to_csv(f'{repo_dir}/Data/Processed/test_text_df.csv', index = False)\n","\n","train_info_df.to_csv(f'{repo_dir}/Data/Processed/train_info_df.csv', index = False)\n","test_info_df.to_csv(f'{repo_dir}/Data/Processed/test_info_df.csv', index = False)"]},{"cell_type":"markdown","metadata":{"id":"n97hmHXxQfbB"},"source":["## 4.Developing the Model(s)"]},{"cell_type":"markdown","metadata":{"id":"MdiG55UjQfbB"},"source":["### 4.1.Text Model"]},{"cell_type":"markdown","metadata":{"id":"vWTl7rIjw8jL"},"source":["#### 4.1.1.Tokenizing \u0026 Defining Functions"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":862,"status":"ok","timestamp":1631042953205,"user":{"displayName":"Keivan Ipchi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiYWJZ-pareZcENvSntLEr0hS7wN6xv1TYBeXeu=s64","userId":"05399847325073645104"},"user_tz":-270},"id":"iHf19YZ2QfbB"},"outputs":[],"source":["tokenizer = Tokenizer(oov_token = '?')\n","tokenizer.fit_on_texts(train_text_df['clean_text'])\n","\n","# Modes: 'binary', 'count', 'freq', 'tfidf'\n","X_text_train_encodes = tokenizer.texts_to_matrix(train_text_df['clean_text'], mode = 'binary')\n","X_text_test_encodes = tokenizer.texts_to_matrix(test_text_df['clean_text'], mode = 'binary')\n","\n","print('Matrix shape:', X_text_train_encodes.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5,"status":"ok","timestamp":1631042953205,"user":{"displayName":"Keivan Ipchi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiYWJZ-pareZcENvSntLEr0hS7wN6xv1TYBeXeu=s64","userId":"05399847325073645104"},"user_tz":-270},"id":"H1MYp3QkQfbB"},"outputs":[],"source":["print('Sentences count:', tokenizer.document_count)\n","print('Top 5 words that are most frequent among vocabulary:', sorted(tokenizer.word_counts.items(), key = lambda x: x[1], reverse = True)[:5])\n","print('Top 5 words that are most frequent among sentences:', sorted(tokenizer.word_docs.items(), key = lambda x: x[1], reverse = True)[:5])"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"qj8GugEvQfbC"},"outputs":[],"source":["def build_model(x, loss, optimizer, metrics):\n","    model = Sequential()\n","    model.add(Dense(units = 64, input_shape = (x.shape[1],), activation = 'relu'))\n","    model.add(Dense(units = 1, activation = 'sigmoid'))\n","    model.compile(loss = loss, optimizer = optimizer, metrics = metrics)\n","\n","    return model"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"eo-QoF13QfbC"},"outputs":[],"source":["def train_model(model, x, y, epochs, batch_size, validation_split):\n","    history = model.fit(\n","    x = x,\n","    y = y,\n","    epochs = epochs,\n","    batch_size = batch_size,\n","    validation_split = validation_split\n","    )\n","\n","    return history.history"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"rcid9zszQfbC"},"outputs":[],"source":["def plot_model_training(epochs, history, metrics):    \n","\n","    def plot_subplot(axs, metric, val_metric):\n","        ''' Plot a single subplot '''\n","\n","        axs.set_title('Analysis of ' + metric)\n","        axs.plot(epochs, history[metric], label = metric)\n","        axs.plot(epochs, history[val_metric], label = val_metric)\n","        axs.legend()\n","\n","    fig, axs = plt.subplots(1, len(metrics), figsize = (18, 5))\n","\n","    for i, metric in enumerate(metrics):\n","        plot_subplot(axs[i], metric, 'val_' + metric)"]},{"cell_type":"markdown","metadata":{"id":"zlWqBp1WQfbC"},"source":["#### 4.1.2.Building, Training and Plotting the Model"]},{"cell_type":"code","execution_count":39,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/","height":312},"executionInfo":{"elapsed":989,"status":"ok","timestamp":1631043270483,"user":{"displayName":"Keivan Ipchi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiYWJZ-pareZcENvSntLEr0hS7wN6xv1TYBeXeu=s64","userId":"05399847325073645104"},"user_tz":-270},"id":"ikbNMUVOQfbC"},"outputs":[],"source":["# Build model\n","model = build_model(\n","  x = X_text_train_encodes,\n","  loss = 'binary_crossentropy',\n","  optimizer = 'adam',\n","  metrics = ['accuracy']\n",")\n","\n","# Plot model\n","plot_model(\n","  model = model,\n","  to_file = f\"{repo_dir}/Data/Models/Text/model.png\",\n","  show_dtype = True,\n","  show_shapes = True,\n","  show_layer_names = False\n",")"]},{"cell_type":"code","execution_count":40,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/","height":353},"executionInfo":{"elapsed":4266,"status":"ok","timestamp":1631043306327,"user":{"displayName":"Keivan Ipchi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiYWJZ-pareZcENvSntLEr0hS7wN6xv1TYBeXeu=s64","userId":"05399847325073645104"},"user_tz":-270},"id":"ziZ6ZkiHQfbC"},"outputs":[],"source":["# Train model\n","history = train_model(\n","  model = model,\n","  x = X_text_train_encodes,\n","  y = train_text_df['target'],\n","  epochs = 1,\n","  batch_size = 64,\n","  validation_split = 0.2\n",")\n","\n","# Plot training process\n","epochs = [i for i in range(len(history['loss']))]\n","plot_model_training(\n","  epochs = epochs,\n","  history = history,\n","  metrics = ['accuracy', 'loss']\n",")"]},{"cell_type":"markdown","metadata":{"id":"spbsvsJoQfbD"},"source":["#### 4.1.2.Making a Prediction"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":560,"status":"ok","timestamp":1631042984015,"user":{"displayName":"Keivan Ipchi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiYWJZ-pareZcENvSntLEr0hS7wN6xv1TYBeXeu=s64","userId":"05399847325073645104"},"user_tz":-270},"id":"v2sfkN-PQfbD"},"outputs":[],"source":["pred = model.predict(X_text_test_encodes, verbose = 2)\n","pred = np.round(pred).astype(int).reshape(pred.shape[0])\n","\n","# Create a submission\n","submission = pd.DataFrame({'id': test_text_df['id'], 'target': pred})\n","submission.to_csv(f'{repo_dir}\\Data\\submission.csv', index = False)"]}],"metadata":{"colab":{"collapsed_sections":["M4LfZGZ-Xfl9","w5Ma28OXQfa5","qiOFwQQHW6tF","6cwzju9hQfa6","w_ueJpbOQfa7","m1t0P7RtQfa8","Oje0q_hjQfa9","YljoV5DQQfa-","MXjMMX8LQfa-","Z4byYE9CbclS","k8AKTgGVQfa_","JFglfCFnQfa_","dMU9ITJNQfbA","5q0IpPyPW9-G","wYisDconQfbC","xwnVf8elQfbC","OGk4upw2QfbC","zlWqBp1WQfbC","spbsvsJoQfbD"],"name":"NLP with Disaster Tweets.ipynb","toc_visible":true,"version":""},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.10"}},"nbformat":4,"nbformat_minor":2}