{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import re                                                       # Regex for string extraction\nimport spacy                                                    # Spacy for text processing\nimport string                                                   # For using string functions and properties\nimport numpy as np                                              # Linear Algebra\nimport pandas as pd                                             # Pandas for DataFrame\nfrom matplotlib import pyplot as plt                            # Matplotlib for plots\nfrom sklearn.model_selection import train_test_split            # Used to split the data into training, validation and test sets\nfrom wordcloud import WordCloud                                 # Wordclouds\n\n# Keras packages\nfrom keras.preprocessing.text import Tokenizer                  # Simplifies different tokenizing methods\nfrom keras.utils.vis_utils import plot_model                    # For plotting model\nfrom keras.models import Sequential                             # Sequential model\nfrom keras.layers import Dense                                  # Dense layer\n\nnlp = spacy.load('en_core_web_sm')  # Load the English language (Other options: en_core_web_sm, en_core_web_md, en_core_web_lg)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-08-01T05:49:20.872191Z","iopub.execute_input":"2021-08-01T05:49:20.873056Z","iopub.status.idle":"2021-08-01T05:49:21.883774Z","shell.execute_reply.started":"2021-08-01T05:49:20.873008Z","shell.execute_reply":"2021-08-01T05:49:21.882778Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Load Data\nLet's load the data and analyse it's primary properties:","metadata":{}},{"cell_type":"code","source":"# Load datasets\ntrain_df = pd.read_csv('../input/nlp-getting-started/train.csv')\ntest_df = pd.read_csv('../input/nlp-getting-started/test.csv')\n\n# Combine them for ease of use\ncombined = [train_df, test_df]\n\n# Take a leak at the data\ntrain_df.head(5)","metadata":{"execution":{"iopub.status.busy":"2021-08-01T05:30:12.471565Z","iopub.execute_input":"2021-08-01T05:30:12.471843Z","iopub.status.idle":"2021-08-01T05:30:12.56135Z","shell.execute_reply.started":"2021-08-01T05:30:12.471816Z","shell.execute_reply":"2021-08-01T05:30:12.560263Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Print basic properties\nprint(f\"- Training Data has {train_df.shape[0]} rows and {train_df.shape[1]} columns: ({list(train_df.columns)})\")\nprint(f\"- Testing Data has {test_df.shape[0]} rows and {test_df.shape[1]} columns: ({list(test_df.columns)})\")","metadata":{"execution":{"iopub.status.busy":"2021-08-01T05:30:12.563196Z","iopub.execute_input":"2021-08-01T05:30:12.563491Z","iopub.status.idle":"2021-08-01T05:30:12.568596Z","shell.execute_reply.started":"2021-08-01T05:30:12.563459Z","shell.execute_reply":"2021-08-01T05:30:12.567865Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Exploratory Data Analysis (EDA)\nIn this step, we analyze different features of our datasets to unidentify abnormalities and their behaviors. We will do the followings:\n- Check the distribution of our targets in both train & test data. (If they are not balanced, we might have to downcast our samples, otherwise our model will be biased!)\n- Investigate how helpful the **Keyword** feature is and how that can be used in the model.\n- Investigate the **Location** feature and check if it is useful for our model.\n- Go through the **Text** features and identify problems and possible fixes. Also look for possible features that can be extracted and analyzed later on.","metadata":{}},{"cell_type":"markdown","source":"### Target feature distrubution","metadata":{}},{"cell_type":"code","source":"# Check the distribution of tagets\ntarget_counts = train_df['target'].value_counts()\n\nfig = plt.figure(figsize = (20, 3))\nplt.title('Target distribution')\nplt.xlabel('Targets')\nplt.ylabel('Counts')\nplt.xticks([0, 1])\nplt.bar(target_counts.keys(), target_counts.values)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-08-01T05:30:12.569964Z","iopub.execute_input":"2021-08-01T05:30:12.570419Z","iopub.status.idle":"2021-08-01T05:30:12.715098Z","shell.execute_reply.started":"2021-08-01T05:30:12.570335Z","shell.execute_reply":"2021-08-01T05:30:12.714099Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Looking at the distribution, I say it's OK. It's not evenly balanced and that can be something we might want to fix by downcasting the data later when optimizing the model. Downcasting may or may not be a good idea as we loose some of our data! (You have to test it to find out its impact on the model)","metadata":{}},{"cell_type":"markdown","source":"### Location feature","metadata":{}},{"cell_type":"code","source":"# NULL entries\nnull_entries = train_df.loc[train_df['location'].isnull()]\nnull_entries_count = len(null_entries)\nprint(f'- There are {null_entries_count} missing entries on \"location\" column ({round(null_entries_count / train_df.shape[0] * 100, 1)}%)')\n\n# Uniqueness\nunique_locations = train_df['location'].unique()\nunique_locations_count = len(unique_locations)\nprint(f'- Number of unique locations: {unique_locations_count} ({round(unique_locations_count/train_df.shape[0]*100, 2)}%)')","metadata":{"execution":{"iopub.status.busy":"2021-08-01T05:30:12.716588Z","iopub.execute_input":"2021-08-01T05:30:12.716962Z","iopub.status.idle":"2021-08-01T05:30:12.728544Z","shell.execute_reply.started":"2021-08-01T05:30:12.716928Z","shell.execute_reply":"2021-08-01T05:30:12.727366Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 20 Top Frequencies\nlocation_freq = train_df['location'].value_counts()\ntop_20_locs = location_freq[:20]\n\nfig = plt.figure(figsize = (20, 3))\nplt.title('20 Most used Locations')\nplt.bar(top_20_locs.index, top_20_locs.values)\nplt.xticks(rotation = 30)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-08-01T05:30:12.729844Z","iopub.execute_input":"2021-08-01T05:30:12.730401Z","iopub.status.idle":"2021-08-01T05:30:12.959143Z","shell.execute_reply.started":"2021-08-01T05:30:12.730366Z","shell.execute_reply":"2021-08-01T05:30:12.958154Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"After careful investigation of the **location** column, we can take note that:\n- Some countries, although the same, have different naming convensions (eg. USA, United states)\n- There are some formatting errors (eg. T E X A S)\n- 33% of the locations are missing. That's a third of the data!\n- Around 44% of the existing locations in the dataset are unique.","metadata":{}},{"cell_type":"markdown","source":"### Keyword feature","metadata":{}},{"cell_type":"code","source":"# NULL entries\nnull_entries = train_df.loc[train_df['keyword'].isnull()]\nnull_entries_count = len(null_entries)\nprint(f'- There are {null_entries_count} missing entries on \"keyword\" column ({round(null_entries_count / train_df.shape[0] * 100, 2)}%)')\n\n# Uniqueness\nunique_locations = train_df['keyword'].unique()\nunique_locations_count = len(unique_locations)\nprint(f'- Number of unique keyword: {unique_locations_count} ({round(unique_locations_count/train_df.shape[0]*100, 2)}%)')","metadata":{"execution":{"iopub.status.busy":"2021-08-01T05:30:12.960391Z","iopub.execute_input":"2021-08-01T05:30:12.960729Z","iopub.status.idle":"2021-08-01T05:30:12.969082Z","shell.execute_reply.started":"2021-08-01T05:30:12.960687Z","shell.execute_reply":"2021-08-01T05:30:12.968107Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 20 Top Frequencies\nkeyword_freq = train_df['keyword'].value_counts()\nkeys_25_locs = keyword_freq[:25]\n\nfig = plt.figure(figsize = (20, 3))\nplt.title('25 Most used Keywords')\nplt.bar(keys_25_locs.index, keys_25_locs.values)\nplt.xticks(rotation = 30)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-08-01T05:30:12.97122Z","iopub.execute_input":"2021-08-01T05:30:12.971538Z","iopub.status.idle":"2021-08-01T05:30:13.221697Z","shell.execute_reply.started":"2021-08-01T05:30:12.971475Z","shell.execute_reply":"2021-08-01T05:30:13.220747Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We understand the followings from a quick analysis of the **keywords** feature:\n- Most keywords contain space which is represented by *%20* which should be fixed.\n- Most keywords indicate distaster signs according to our graph. Good sign here!\n- Approximatly 0.8% entries with missing *Keyword* which is also a good sign!\n- Only about 3% of the keywords are unique.","metadata":{}},{"cell_type":"markdown","source":"### Text feature","metadata":{}},{"cell_type":"markdown","source":"For the sake of practice, I am going to extract as much information as I can from the text, this includes extracting links, hashtags and mentions and anything that can be considered useful. Some of the tasks you might want to do:\n- There are lot's of #hashtags used that can be helpful, so extract and store them in a list for each tweet.\n- There also are some links, however, since links are shortened (encoded), they are useless in their basic form. We could scrape each link for headlines and titles, but I'm going to remove them for sipmplicity.\n- @mentions can be seen in some tweets, maybe they point to a certain user or something, extract the username and store it in a list for each tweet for later analysis.\n- There are some emojies and None-ASCII characters that are better be removed.\n- Numbers can also be removed as they may be a little misleading.\n- Some comments are fully written in uppercase, we may want to make them lowercase; Or there are words that don't have to be in upper form, like ALLAH.\n- There are comments that are almost identical except their links, we can easily detect and remove the duplicates after extracting links (See text[110:120])\n- Maybe finding the common words can be useful for identifing which class does each tweet belong to.\n- There are some typos in some texts, they can be fixed with some python packages.\n- We could extract emojis!","metadata":{}},{"cell_type":"code","source":"train_df['text'].iloc[:50].values","metadata":{"execution":{"iopub.status.busy":"2021-08-01T05:31:11.054828Z","iopub.execute_input":"2021-08-01T05:31:11.055195Z","iopub.status.idle":"2021-08-01T05:31:11.063009Z","shell.execute_reply.started":"2021-08-01T05:31:11.055161Z","shell.execute_reply":"2021-08-01T05:31:11.06187Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Feature Extraction","metadata":{}},{"cell_type":"markdown","source":"We extract linkes, hashtags and mentions from the tweet texts and store them  in the dataset as lists. By a more careful analysis, it can be shown that links have been shortened. You could scrape each link and search for topics and healines but since this is time consuming and there aren't many links in the dataset, I decided to use a more simple approach and remove them all.\n\nI also found @mentions useless as they are usernames and usernames can be pretty meaningless and made of random characters. Let's not over-complicate the analysis and remove them as well.\n\nBut #hashtags are still very useful as they are used to tag topics and can be used to group tweets.","metadata":{}},{"cell_type":"code","source":"def extract_hashtag(text):\n    matches = re.findall(r'#\\w*', text)\n    return [match[1:].lower() for match in matches] if matches != [] else None\n\ndef extract_mention(text):\n    matches = re.findall(r'@\\w*', text)\n    return [match[1:].lower() for match in matches] if matches != [] else None\n\ndef extract_links(text):\n    matches = re.findall(r'\\bhttps?://\\S+', text)\n    return matches if matches != [] else None","metadata":{"execution":{"iopub.status.busy":"2021-08-01T05:32:33.256119Z","iopub.execute_input":"2021-08-01T05:32:33.256448Z","iopub.status.idle":"2021-08-01T05:32:33.262601Z","shell.execute_reply.started":"2021-08-01T05:32:33.256417Z","shell.execute_reply":"2021-08-01T05:32:33.261504Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create two new features for #hashtags, @mentions and links\nfor df in combined:\n  df['hashtags'] = df['text'].apply(extract_hashtag)\n  df['mentions'] = df['text'].apply(extract_mention)\n  df['links'] = df['text'].apply(extract_links)","metadata":{"execution":{"iopub.status.busy":"2021-08-01T05:32:45.172367Z","iopub.execute_input":"2021-08-01T05:32:45.172855Z","iopub.status.idle":"2021-08-01T05:32:45.256781Z","shell.execute_reply.started":"2021-08-01T05:32:45.17281Z","shell.execute_reply":"2021-08-01T05:32:45.255877Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"feature_counts = train_df.count()\n\nfig = plt.figure(figsize = (20, 3))\nplt.title('Features counts')\nplt.xlabel('Feature')\nplt.ylabel('Count')\nplt.bar(feature_counts.keys(), feature_counts.values)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-08-01T05:33:02.659845Z","iopub.execute_input":"2021-08-01T05:33:02.660189Z","iopub.status.idle":"2021-08-01T05:33:02.820941Z","shell.execute_reply.started":"2021-08-01T05:33:02.660159Z","shell.execute_reply":"2021-08-01T05:33:02.81996Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As you can see, many hastags, mentions and links are missing already. They aren't much beneficial now, sadly.\nThat brings us to our first place, with no new features to work with. But that's ok because that was necessary to get us to the point where we can be confident in our decisions about removing them from our text data.","metadata":{}},{"cell_type":"markdown","source":"#### Wordclouds","metadata":{}},{"cell_type":"code","source":"# Word cloud for disaster tweets\nplt.figure(figsize = (10, 8))\nword_cloud = WordCloud(max_font_size = 50).generate(\" \".join(train_df.loc[train_df['target'] == 1, 'text'].iloc[:100]))\nplt.imshow(word_cloud)\nplt.axis('off')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-08-01T05:50:02.812394Z","iopub.execute_input":"2021-08-01T05:50:02.812825Z","iopub.status.idle":"2021-08-01T05:50:03.154694Z","shell.execute_reply.started":"2021-08-01T05:50:02.812786Z","shell.execute_reply":"2021-08-01T05:50:03.153789Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Word cloud for non-disaster tweets\nplt.figure(figsize = (10, 8))\nword_cloud = WordCloud(max_font_size = 50).generate(\" \".join(train_df.loc[train_df['target'] == 0, 'text'].iloc[:100]))\nplt.imshow(word_cloud)\nplt.axis('off')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-08-01T05:50:09.029459Z","iopub.execute_input":"2021-08-01T05:50:09.030018Z","iopub.status.idle":"2021-08-01T05:50:09.447439Z","shell.execute_reply.started":"2021-08-01T05:50:09.029964Z","shell.execute_reply":"2021-08-01T05:50:09.446037Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Data Cleaning","metadata":{}},{"cell_type":"markdown","source":"This is a quite important step. After randomly looking at out text data, I found it usefull to do the followings:\n- Remove all links\n- Remove all HTML tags\n- Remove Emojis (Using their unicode)\n- Remove all @mentions\n- Remove all non-ASCII characters\n- Lemmatize all words (This is a very important step, since lemmatization is a way to reduce the number of words in the text)\n- Remove all stop words (Most stop words don't provided much information)\n- Remove all punctuations (Careful with this one, since it might remove important information that can be used in Embeddings later on...)\n- Conver all words to lower case (Danagerous for Embeddings! - \"MIT\" is a university in US, but \"mit\" means \"with\" in germany)\n- Fix text case (Some comments are fully uppercase, it's a good practice to convert them into lowercase)","metadata":{}},{"cell_type":"code","source":"# Remove URLs\ndef remove_urls(text):\n    return re.sub(r'https?://\\S+|www\\.\\S+', '', text)\n\n# Remove HTML\ndef remove_html(text):\n    return re.sub(r'<.*?>', '', text)\n\n# Converts text to lowercase\ndef to_lower(text):\n  return text.lower()\n\n# Remove numbers\ndef remove_numbers(text):\n    return re.sub(r'\\d+', '', text)\n\n# Remove mentions\ndef remove_mentions(text):\n    return re.sub(r'@\\w*', '', text)\n\n# Remove emojis\ndef remove_emojis(text):\n  regrex_pattern = re.compile(pattern = \"[\"\n    u\"\\U0001F600-\\U0001F64F\"\n    u\"\\U0001F300-\\U0001F5FF\"\n    u\"\\U0001F680-\\U0001F6FF\"\n    u\"\\U0001F1E0-\\U0001F1FF\"\n    \"]+\", flags = re.UNICODE)\n  return regrex_pattern.sub(r'',text)\n\n# Remove non-ASCII characters\ndef remove_non_ascii(text):\n    return ''.join(filter(lambda x: x in string.printable, text))\n\n# Lemmatize text\ndef lemmatize_text(text):\n    return ' '.join([token.lemma_ for token in nlp(text)])\n\n# Remove stopwords\ndef remove_stopwords(text):\n    return ' '.join([token.text for token in nlp(text) if not token.is_stop])\n\n# Remove punctuation\ndef remove_punctuation(text):\n    table = str.maketrans('', '', string.punctuation)\n    return text.translate(table)\n\n# Remove white space (Extra step, just in case)\ndef remove_whitespace(text):\n    return ' '.join(text.split())\n\ndef clean_text(text, lower = True,html = True,urls = True, numbers = True, mentions = True, ascii = True,\n               punctuations = True, stopwords = True, lemmatize = True):\n    ''' Properties to remove when cleaning the given text'''    \n\n    # Clean text\n    if lower:\n      text = to_lower(text)\n    if html:\n      text = remove_html(text)\n    if urls:\n      text = remove_urls(text)\n    if numbers:\n      text = remove_numbers(text)\n    if mentions:\n      text = remove_mentions(text)\n    if ascii:\n      text = remove_non_ascii(text)\n    if punctuations:\n      text = remove_punctuation(text)\n    if stopwords:\n      text = remove_stopwords(text)\n    if lemmatize:\n      text = lemmatize_text(text)\n\n    return remove_whitespace(text)      # Last step just in case","metadata":{"execution":{"iopub.status.busy":"2021-08-01T05:35:07.453011Z","iopub.execute_input":"2021-08-01T05:35:07.453363Z","iopub.status.idle":"2021-08-01T05:35:07.465617Z","shell.execute_reply.started":"2021-08-01T05:35:07.453325Z","shell.execute_reply":"2021-08-01T05:35:07.464574Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for df in combined:\n  df['cleaned_text'] = df['text'].apply(clean_text)\n\ntrain_df.head()","metadata":{"execution":{"iopub.status.busy":"2021-08-01T05:35:24.116577Z","iopub.execute_input":"2021-08-01T05:35:24.116938Z","iopub.status.idle":"2021-08-01T05:38:05.109767Z","shell.execute_reply.started":"2021-08-01T05:35:24.116905Z","shell.execute_reply":"2021-08-01T05:38:05.108767Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Deep Learning Model","metadata":{}},{"cell_type":"code","source":"# Drop records with no texts from training data, replace the emoty texts for test data with '?'\ntrain_df.dropna(axis = 0, subset = ['cleaned_text'], inplace = True)\ntest_df['cleaned_text'].fillna('?', inplace = True)\n\n# Shuffle\ntrain_df = train_df.sample(frac = 1).reset_index(drop = True)\n\ntrain_df.info()","metadata":{"execution":{"iopub.status.busy":"2021-08-01T05:58:48.238264Z","iopub.execute_input":"2021-08-01T05:58:48.238676Z","iopub.status.idle":"2021-08-01T05:58:48.273981Z","shell.execute_reply.started":"2021-08-01T05:58:48.238622Z","shell.execute_reply":"2021-08-01T05:58:48.273181Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Encoding","metadata":{}},{"cell_type":"code","source":"tokenizer = Tokenizer(lower = True, oov_token = '?')\ntokenizer.fit_on_texts(train_df['cleaned_text'])\n\n# Modes: 'binary', 'count', 'freq', 'tfidf'\ntrain_encodes = tokenizer.texts_to_matrix(train_df['cleaned_text'], mode = 'count')\ntest_encodes = tokenizer.texts_to_matrix(test_df['cleaned_text'], mode = 'count')\n\nprint('Matrix shape:', train_encodes.shape)","metadata":{"execution":{"iopub.status.busy":"2021-08-01T06:01:09.824714Z","iopub.execute_input":"2021-08-01T06:01:09.825351Z","iopub.status.idle":"2021-08-01T06:01:10.460119Z","shell.execute_reply.started":"2021-08-01T06:01:09.8253Z","shell.execute_reply":"2021-08-01T06:01:10.459326Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Building the Model","metadata":{}},{"cell_type":"code","source":"def build_model(x, loss, optimizer, metrics):\n  model = Sequential()\n  model.add(Dense(units = 64, input_shape = (x.shape[1],), activation = 'relu'))\n  model.add(Dense(units = 1, activation = 'sigmoid'))\n  model.compile(loss = loss, optimizer = optimizer, metrics = metrics)  \n\n  return model","metadata":{"execution":{"iopub.status.busy":"2021-08-01T06:01:33.053372Z","iopub.execute_input":"2021-08-01T06:01:33.053883Z","iopub.status.idle":"2021-08-01T06:01:33.059581Z","shell.execute_reply.started":"2021-08-01T06:01:33.053847Z","shell.execute_reply":"2021-08-01T06:01:33.05849Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Training the Model","metadata":{}},{"cell_type":"code","source":"def train_model(model, x, y, epochs, batch_size, validation_split):\n  history = model.fit(\n    x = x,\n    y = y,\n    epochs = epochs,\n    batch_size = batch_size,\n    validation_split = validation_split\n  )\n\n  return history.history","metadata":{"execution":{"iopub.status.busy":"2021-08-01T06:01:46.473808Z","iopub.execute_input":"2021-08-01T06:01:46.474177Z","iopub.status.idle":"2021-08-01T06:01:46.478459Z","shell.execute_reply.started":"2021-08-01T06:01:46.474141Z","shell.execute_reply":"2021-08-01T06:01:46.47774Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Plotting the Model","metadata":{}},{"cell_type":"code","source":"def plot_model_training(epochs, history, metrics):    \n\n  def plot_subplot(axs, metric, val_metric):\n    ''' Plot a single subplot '''\n\n    axs.set_title('Analysis of ' + metric)\n    axs.plot(epochs, history[metric], label = metric)\n    axs.plot(epochs, history[val_metric], label = val_metric)\n    axs.legend()\n\n  fig, axs = plt.subplots(1, len(metrics), figsize = (18, 5))\n\n  for i, metric in enumerate(metrics):\n    plot_subplot(axs[i], metric, 'val_' + metric)","metadata":{"execution":{"iopub.status.busy":"2021-08-01T06:01:56.432278Z","iopub.execute_input":"2021-08-01T06:01:56.432798Z","iopub.status.idle":"2021-08-01T06:01:56.438389Z","shell.execute_reply.started":"2021-08-01T06:01:56.432752Z","shell.execute_reply":"2021-08-01T06:01:56.437387Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### All in One","metadata":{}},{"cell_type":"code","source":"X_train = train_encodes\ny_train = train_df['target']\n\n# Build model\nmodel = build_model(\n  x = X_train,\n  loss = 'binary_crossentropy',\n  optimizer = 'adam',\n  metrics = ['accuracy']\n)\n\n# Plot model\nplot_model(model = model, show_dtype = True, show_shapes = True, show_layer_names = False)\n\n# Train model\nhistory = train_model(\n  model = model,\n  x = X_train,\n  y = y_train,\n  epochs = 1,\n  batch_size = 64,\n  validation_split = 0.2\n)\n\n# Plot training process\nepochs = [i for i in range(len(history['loss']))]\nplot_model_training(\n  epochs = epochs,\n  history = history,\n  metrics = ['accuracy', 'loss']\n)","metadata":{"execution":{"iopub.status.busy":"2021-08-01T06:02:08.497199Z","iopub.execute_input":"2021-08-01T06:02:08.497744Z","iopub.status.idle":"2021-08-01T06:02:11.805175Z","shell.execute_reply.started":"2021-08-01T06:02:08.497694Z","shell.execute_reply":"2021-08-01T06:02:11.804316Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Make a Prediction","metadata":{}},{"cell_type":"code","source":"X_test = test_encodes\n\npred = model.predict(X_test, verbose = 2)\npred = np.round(pred).astype(int).reshape(pred.shape[0])\n\n# Create submission\nsubmission = pd.DataFrame({'id': test_df['id'].values.tolist(), 'target': pred})\nsubmission.to_csv('./submission.csv', index = False)","metadata":{"execution":{"iopub.status.busy":"2021-08-01T06:02:55.09346Z","iopub.execute_input":"2021-08-01T06:02:55.094103Z","iopub.status.idle":"2021-08-01T06:02:55.521361Z","shell.execute_reply.started":"2021-08-01T06:02:55.094063Z","shell.execute_reply":"2021-08-01T06:02:55.520543Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"kaggle competitions submit -c nlp-getting-started -f submission.csv","metadata":{"execution":{"iopub.status.busy":"2021-08-01T06:06:39.163257Z","iopub.execute_input":"2021-08-01T06:06:39.163663Z","iopub.status.idle":"2021-08-01T06:06:39.170851Z","shell.execute_reply.started":"2021-08-01T06:06:39.163618Z","shell.execute_reply":"2021-08-01T06:06:39.16948Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}